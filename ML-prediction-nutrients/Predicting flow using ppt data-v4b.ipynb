{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The goal of this project file is to predict flow using ppt data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version 4: \n",
    "- instead of systematically adding/removing each variable to achieve the best r2 values, we are randomly selecting a set of predictors (n = 4, 5, 6....10, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import sklearn.datasets as skd\n",
    "import sklearn.ensemble as ske\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime as dt\n",
    "from pprint import pprint\n",
    "import random\n",
    "import xlsxwriter\n",
    "from openpyxl.styles import Font\n",
    "from openpyxl import Workbook\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing flow dataset\n",
    "df_flow = pd.read_excel('Daily ISCO flow and precipitation.xlsx', sheet_name = 'Sheet1')\n",
    "\n",
    "\n",
    "df_flow = df_flow.drop(columns=['Daily precip (in)','Daily precip (mm)','S12-ISCO flow (cms)','T12-ISCO flow (cms)'])\n",
    "\n",
    "df_flow = df_flow.rename(columns={'T8-ISCO flow (cms)':'Flow8', 'S11-ISCO flow (cms)':'Flow11', 'S12+T12':'Flow12'})\n",
    "\n",
    "#df_flow = pd.DataFrame(df_flow, columns=['Sample date', 'Sample type', 'Site', 'Flow'])\n",
    "df_flow.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing weather dataset\n",
    "df_weather = pd.read_excel('Weather data.xlsx', sheet_name = 'Weather')\n",
    "df_weather = df_weather.rename(columns={'ppt (mm)':'ppt','tmin (degrees C)':'tmin','tmean (degrees C)':'tmean',\n",
    "                                        'tmax (degrees C)':'tmax', 'tdmean (degrees C)':'tdew', 'vpdmin (hPa)': 'vpdmin', \n",
    "                                        'vpdmax (hPa)': 'vpdmax'})\n",
    "\n",
    "df_weather['Year'] = pd.DatetimeIndex(df_weather['Date']).year\n",
    "df_weather['Month'] = pd.DatetimeIndex(df_weather['Date']).month\n",
    "df_weather['Day'] = pd.DatetimeIndex(df_weather['Date']).day\n",
    "df_weather['Day of year'] = df_weather['Date'].dt.dayofyear\n",
    "\n",
    "# calculate antecedent ppt and temperature\n",
    "# 2-day antecedent cumulative ppt (mm)\n",
    "df_weather['2-day ppt'] = round(df_weather.iloc[:,1].rolling(window=2).sum(), 1)\n",
    "\n",
    "# 3-day antecedent cumulative ppt (mm)\n",
    "df_weather['3-day ppt'] = round(df_weather.iloc[:,1].rolling(window=3).sum(), 1)\n",
    "\n",
    "# 5-day antecedent cumulative ppt (mm)\n",
    "df_weather['5-day ppt'] = round(df_weather.iloc[:,1].rolling(window=5).sum(), 1)\n",
    "\n",
    "# 7-day antecedent cumulative ppt (mm)\n",
    "df_weather['7-day ppt'] = round(df_weather.iloc[:,1].rolling(window=7).sum(), 1)\n",
    "\n",
    "# 15-day antecedent cumulative ppt (mm)\n",
    "df_weather['15-day ppt'] = round(df_weather.iloc[:,1].rolling(window=15).sum(), 1)\n",
    "\n",
    "# 30-day antecedent cumulative ppt (mm)\n",
    "df_weather['30-day ppt'] = round(df_weather.iloc[:,1].rolling(window=30).sum(), 1)\n",
    "\n",
    "# 90-day antecedent cumulative ppt (mm)\n",
    "df_weather['90-day ppt'] = round(df_weather.iloc[:,1].rolling(window=90).sum(), 1)\n",
    "\n",
    "# 180-day antecedent cumulative ppt (mm)\n",
    "df_weather['180-day ppt'] = round(df_weather.iloc[:,1].rolling(window=180).sum(), 1)\n",
    "\n",
    "# 360-day antecedent cumulative ppt (mm)\n",
    "df_weather['360-day ppt'] = round(df_weather.iloc[:,1].rolling(window=360).sum(), 1)\n",
    "\n",
    "# annual and prev annual cumulative ppt (mm)\n",
    "df_annual_ppt = df_weather.groupby(['Year'])['ppt'].sum()\n",
    "df_annual_ppt = df_annual_ppt.to_frame().reset_index()\n",
    "df_annual_ppt = df_annual_ppt.rename(columns={'ppt':'Annual ppt'})\n",
    "df_annual_ppt['Prev annual ppt'] = df_annual_ppt['Annual ppt'].shift(1)\n",
    "df_weather = pd.merge(df_weather, df_annual_ppt, left_on='Year', right_on='Year', how='left')\n",
    "\n",
    "\n",
    "# 2-day antecedent avg temperature (ºC)\n",
    "df_weather['2-day temp'] = round(df_weather.iloc[:,3].rolling(window=2).mean(), 1)\n",
    "\n",
    "# 3-day antecedent avg temperature (ºC)\n",
    "df_weather['3-day temp'] = round(df_weather.iloc[:,3].rolling(window=3).mean(), 1)\n",
    "\n",
    "# 5-day antecedent avg temperature (ºC)\n",
    "df_weather['5-day temp'] = round(df_weather.iloc[:,3].rolling(window=5).mean(), 1)\n",
    "\n",
    "# 7-day antecedent avg temperature (ºC)\n",
    "df_weather['7-day temp'] = round(df_weather.iloc[:,3].rolling(window=7).mean(), 1)\n",
    "\n",
    "# 15-day antecedent avg temperature (ºC)\n",
    "df_weather['15-day temp'] = round(df_weather.iloc[:,3].rolling(window=15).mean(), 1)\n",
    "\n",
    "# 30-day antecedent avg temperature (ºC)\n",
    "df_weather['30-day temp'] = round(df_weather.iloc[:,3].rolling(window=30).mean(), 1)\n",
    "\n",
    "df_weather = df_weather.drop(columns=['tmin','tmax','tdew','vpdmin','vpdmax'])\n",
    "df_train = df_weather[df_weather[\"Year\"].isin([2015, 2016, 2017])]\n",
    "\n",
    "#df_weather.head()\n",
    "\n",
    "# merging flow and weather datasets\n",
    "df_merged = pd.merge(df_train, df_flow, left_on='Date', right_on='Date', how='right')\n",
    "df_merged = df_merged.dropna()\n",
    "\n",
    "# write dataframe to excel\n",
    "#writer = pd.ExcelWriter('Weather data processed.xlsx')\n",
    "#df_merged.to_excel(writer)\n",
    "#writer.save()\n",
    "\n",
    "#df_merged.tail()\n",
    "#df_merged.describe()\n",
    "print(df_merged.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import explained_variance_score, mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# prompt user input to identify which subwatershed to work on\n",
    "print ('which site to analyze?')\n",
    "print ('Options are: 8, 11, 12')\n",
    "sub = input()\n",
    "print ('subwatershed ', sub, ' is selected')\n",
    "\n",
    "sub_flow = 'Flow' + sub\n",
    "y = df_merged[sub_flow]\n",
    "\n",
    "# convert Sample date from timestamp to numeric because sklearn cannot process timestamp format\n",
    "df_merged['Date'] = pd.to_numeric(pd.to_datetime(df_merged['Date']))\n",
    "\n",
    "# look up all the variables, and remove uncessary variables\n",
    "all_variables = list(df_merged.columns)\n",
    "unwanted = {'Date', 'Year', 'Flow8', 'Flow11', 'Flow12'}\n",
    "all_variables = [e for e in all_variables if e not in unwanted]\n",
    "\n",
    "# create an empty list\n",
    "variables = []\n",
    "good_accuracy = float('-inf')\n",
    "better_accuracy = float('-inf')\n",
    "best_accuracy = float('-inf')\n",
    "r2_prev = 0\n",
    "r2 = -1\n",
    "good_r2 = 0\n",
    "better_r2 = 0\n",
    "best_r2 = 0\n",
    "good_var = 0\n",
    "better_var = 0\n",
    "best_var = 0\n",
    "good_avg_error = float('-inf')\n",
    "better_avg_error = float('-inf')\n",
    "best_avg_error = float('-inf')\n",
    "good_med_error = float('-inf')\n",
    "better_med_error = float('-inf')\n",
    "best_med_error = float('-inf')\n",
    "good_df_predict = pd.DataFrame()\n",
    "better_df_predict = pd.DataFrame()\n",
    "best_df_predict = pd.DataFrame()\n",
    "\n",
    "# set min and max numbers of variables in the prediction model\n",
    "min_var_no = 6 \n",
    "max_var_no = 12\n",
    "\n",
    "# this is for random combination approach - set number of random sets of combination of variables in the prediction model\n",
    "combo_no = 1000\n",
    "\n",
    "for var_no in range (min_var_no, max_var_no):\n",
    "    \n",
    "    count = 0    \n",
    "    while r2 < 0.7 and count < combo_no:\n",
    "        class color:\n",
    "            BLUE = '\\033[94m'\n",
    "            BOLD = '\\033[1m'\n",
    "            END = '\\033[0m'\n",
    "        print(color.BOLD + color.BLUE + 'NEW LOOP' + color.END)\n",
    "        \n",
    "        # randomly select variables\n",
    "        variables = random.sample(all_variables, var_no)\n",
    "        \n",
    "        X = df_merged[variables]\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split (X, y, test_size = 0.2, random_state = 0)\n",
    "        rf_base = ske.RandomForestRegressor(n_estimators = 1000, random_state = 0)\n",
    "        rf_base.fit(X_train, Y_train)\n",
    "        Y_pred = rf_base.predict(X_test)\n",
    "\n",
    "        #print('training period explained variance:', round(explained_variance_score(Y_test, Y_pred),2))\n",
    "        #print('training period mean abs error:', round(mean_absolute_error(Y_test, Y_pred),2))\n",
    "        #print('training period mean squared error:', round(mean_squared_error(Y_test, Y_pred),2))\n",
    "        #print('training period r2:', round(r2_score(Y_test, Y_pred),2))\n",
    "\n",
    "        fet_ind = np.argsort(rf_base.feature_importances_)[::-1]\n",
    "        fet_imp = rf_base.feature_importances_[np.argsort(rf_base.feature_importances_)][::-1]\n",
    "\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(8,3))\n",
    "        labels = np.asarray(X_train.columns[fet_ind])\n",
    "        pd.Series(fet_imp, index = labels).plot('bar', ax=ax)\n",
    "        ax.set_title('Feature importance')\n",
    "        plt.show()\n",
    "\n",
    "        df_weathersub = df_weather[df_weather[\"Year\"].isin([2018])]\n",
    "        df_weathersub = df_weathersub.reset_index(drop=True)\n",
    "        df_test = df_weathersub\n",
    "        df_test = df_test[variables]\n",
    "        df_test = df_test.dropna()\n",
    "        df_test = df_test.reset_index(drop=True)\n",
    "\n",
    "        # If 'Date' variable is present, convert the date from timestamp to numeric because sklearn cannot process timestamp format\n",
    "        if 'Date' in list(df_test):\n",
    "            df_test['Date'] = pd.to_numeric(pd.to_datetime(df_test['Date']))\n",
    "\n",
    "        # use base or random depending on model performance\n",
    "        #Y_pred = rf_base.predict(df_test)\n",
    "        Y_pred = rf_base.predict(df_test)\n",
    "\n",
    "        df_predict = pd.DataFrame(Y_pred, columns=['est. flow'])\n",
    "        df_predict['est. flow'] = round(df_predict['est. flow'], 3)\n",
    "        df_predict = pd.merge(df_test, df_predict, left_index=True, right_index=True)\n",
    "        df_predict = pd.merge(df_weathersub['Date'], df_predict, left_index=True, right_index=True)\n",
    "        df_predict = df_predict.rename(columns={'Date_x':'Date', 'Date_y':'Date num'})\n",
    "\n",
    "        #df_predict['Date'] = pd.to_datetime(df_predict['Date'])\n",
    "\n",
    "        # this is ISCO daily avg flow\n",
    "        df_actual = df_flow[['Date', sub_flow]]\n",
    "        df_actual['Year'] = pd.DatetimeIndex(df_actual['Date']).year\n",
    "        df_actual = df_actual[df_actual[\"Year\"].isin([2018])]\n",
    "        df_actual = df_actual.dropna()\n",
    "\n",
    "        df_predict = pd.merge(df_predict, df_actual, left_on='Date', right_on='Date', how='right')\n",
    "        df_predict['error (%)'] = round(((df_predict[sub_flow]-df_predict['est. flow'])/df_predict[sub_flow]*100), 0)\n",
    "\n",
    "        plt.plot(df_predict['Date'], df_predict['est. flow'], linestyle = 'dotted', label='estimated')\n",
    "        plt.xlabel('Date')\n",
    "        plt.xticks(rotation = 30)\n",
    "        plt.plot(df_predict['Date'], df_predict[sub_flow], linestyle = 'dotted', label='measured')\n",
    "        plt.ylabel('Estimated flow (cms)')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        #print('prediction period explained variance:',round(explained_variance_score(df_predict[sub_flow], df_predict['est. flow']),2))\n",
    "        #print('prediction period mean abs error:', round(mean_absolute_error(df_predict[sub_flow], df_predict['est. flow']),2))\n",
    "        #print('prediction period mean squared error:', round(mean_squared_error(df_predict[sub_flow], df_predict['est. flow']),2))\n",
    "        #print('prediction period r2:', round(r2_score(df_predict[sub_flow], df_predict['est. flow']),2))\n",
    "\n",
    "        plt.boxplot(df_predict['error (%)'])\n",
    "        plt.ylabel('Error in est. flow (%)')\n",
    "        plt.show()\n",
    "                \n",
    "        r2 = round(r2_score(df_predict[sub_flow], df_predict['est. flow']),2)\n",
    "        abs_error = abs(df_predict[sub_flow] - df_predict['est. flow'])\n",
    "        # ignore infinity value (when anything divided by 0 cfs flow = infinity) when calculating mean\n",
    "        pe = abs_error / df_predict[sub_flow]\n",
    "        mape = 100* np.mean(pe[np.isfinite(pe)])\n",
    "        accuracy = round(100 - mape, 1)        \n",
    "        avg_error = round(df_predict['error (%)'].mean(),2)\n",
    "        med_error = round(df_predict['error (%)'].median(),2) \n",
    "                   \n",
    "        print('R2 value: ', r2)\n",
    "        print('Median accuracy: ', accuracy, '%')\n",
    "        print('Variables included:', variables)\n",
    "        print('Average error:', round(df_predict['error (%)'].mean(),2))\n",
    "        print('Median error:', round(df_predict['error (%)'].median(),2))     \n",
    "           \n",
    "        # store the best variables and its model outputs (based on r2)\n",
    "        # consider setting the \"best\" variables based on other parameters, such as accuracy and r2\n",
    "        if r2 >= good_r2:\n",
    "            good_var = variables\n",
    "            good_r2 = r2\n",
    "            good_accuracy = accuracy\n",
    "            good_avg_error = avg_error\n",
    "            good_med_error = med_error           \n",
    "            good_df_predict = df_predict\n",
    "           \n",
    "            if good_r2 >= better_r2:\n",
    "                prev_better_var = better_var\n",
    "                better_var = good_var\n",
    "                good_var = prev_better_var\n",
    "                \n",
    "                prev_better_r2 = better_r2\n",
    "                better_r2 = good_r2\n",
    "                good_r2 = prev_better_r2\n",
    "                \n",
    "                prev_better_accuracy = better_accuracy\n",
    "                better_accuracy = good_accuracy\n",
    "                good_accuracy = prev_better_accuracy\n",
    "                \n",
    "                prev_better_avg_error = better_avg_error\n",
    "                better_avg_error = good_avg_error\n",
    "                good_avg_error = prev_better_avg_error\n",
    "                \n",
    "                prev_better_med_error = better_med_error\n",
    "                better_med_error = good_med_error\n",
    "                good_med_error = prev_better_med_error\n",
    "                \n",
    "                prev_better_df_predict = better_df_predict\n",
    "                better_df_predict = good_df_predict\n",
    "                good_df_predict = prev_better_df_predict\n",
    "                \n",
    "                if better_r2 >= best_r2:\n",
    "                    prev_best_var = best_var\n",
    "                    best_var = better_var\n",
    "                    better_var = prev_best_var\n",
    "\n",
    "                    prev_best_r2 = best_r2\n",
    "                    best_r2 = better_r2\n",
    "                    better_r2 = prev_best_r2\n",
    "\n",
    "                    prev_best_accuracy = best_accuracy\n",
    "                    best_accuracy = better_accuracy\n",
    "                    better_accuracy = prev_best_accuracy\n",
    "\n",
    "                    prev_best_avg_error = best_avg_error\n",
    "                    best_avg_error = better_avg_error\n",
    "                    better_avg_error = prev_best_avg_error\n",
    "\n",
    "                    prev_best_med_error = best_med_error\n",
    "                    best_med_error = better_med_error\n",
    "                    better_med_error = prev_best_med_error\n",
    "\n",
    "                    prev_best_df_predict = best_df_predict\n",
    "                    best_df_predict = better_df_predict\n",
    "                    better_df_predict = prev_best_df_predict\n",
    "\n",
    "        r2_prev = r2\n",
    "        count = count + 1\n",
    "        print('no. of loop: ', count)\n",
    "    \n",
    "    print('no. of variable: ', var_no)\n",
    "\n",
    "# set the excel worksheets and figures export destination to Output folder\n",
    "save_path = \"Outputs/\"     \n",
    "    \n",
    "class color:\n",
    "    BLUE = '\\033[94m'\n",
    "    BOLD = '\\033[1m'\n",
    "    END = '\\033[0m'\n",
    "print(color.BOLD + color.BLUE + 'RESULTS' + color.END)\n",
    "\n",
    "print('GOOD')\n",
    "\n",
    "plt.plot(good_df_predict['Date'], good_df_predict['est. flow'], linestyle = 'dotted', label='estimated')\n",
    "plt.xlabel('Date')\n",
    "plt.xticks(rotation = 30)\n",
    "plt.plot(good_df_predict['Date'], good_df_predict[sub_flow], linestyle = 'dotted', label='measured')\n",
    "plt.ylabel('Estimated flow (cms)')\n",
    "plt.legend()\n",
    "plt.title('Good')\n",
    "plt.savefig(save_path + 'Est. vs measured flow - ' + sub + ' good.png')\n",
    "plt.show()\n",
    "\n",
    "plt.boxplot(good_df_predict['error (%)'])\n",
    "plt.ylabel('Error in est. flow (%)')\n",
    "plt.title('Good')\n",
    "plt.savefig(save_path + 'Est. error - ' + sub + ' good.png')\n",
    "plt.show()\n",
    "\n",
    "print('Good variables: ', good_var)\n",
    "print('Good r2: ', good_r2)\n",
    "print('Good accuracy: ', good_accuracy, '%')\n",
    "print('Good avg error: ', good_avg_error, '%')\n",
    "print('Good median error: ', good_med_error, '%')\n",
    "\n",
    "print('BETTER')\n",
    "\n",
    "plt.plot(better_df_predict['Date'], better_df_predict['est. flow'], linestyle = 'dotted', label='estimated')\n",
    "plt.xlabel('Date')\n",
    "plt.xticks(rotation = 30)\n",
    "plt.plot(better_df_predict['Date'], better_df_predict[sub_flow], linestyle = 'dotted', label='measured')\n",
    "plt.ylabel('Estimated flow (cms)')\n",
    "plt.legend()\n",
    "plt.title('Better')\n",
    "plt.savefig(save_path + 'Est. vs measured flow - ' + sub + ' better.png')\n",
    "plt.show()\n",
    "\n",
    "plt.boxplot(better_df_predict['error (%)'])\n",
    "plt.ylabel('Error in est. flow (%)')\n",
    "plt.title('Better')\n",
    "plt.savefig(save_path + 'Est. error - ' + sub + ' better.png')\n",
    "plt.show()\n",
    "\n",
    "print('Better variables: ', better_var)\n",
    "print('Better r2: ', better_r2)\n",
    "print('Better accuracy: ', better_accuracy, '%')\n",
    "print('Better avg error: ', better_avg_error, '%')\n",
    "print('Better median error: ', better_med_error, '%')\n",
    "\n",
    "print('BEST')\n",
    "\n",
    "plt.plot(best_df_predict['Date'], best_df_predict['est. flow'], linestyle = 'dotted', label='estimated')\n",
    "plt.xlabel('Date')\n",
    "plt.xticks(rotation = 30)\n",
    "plt.plot(best_df_predict['Date'], best_df_predict[sub_flow], linestyle = 'dotted', label='measured')\n",
    "plt.ylabel('Est. flow (cms)')\n",
    "plt.legend()\n",
    "plt.title('Best')\n",
    "plt.savefig(save_path + 'Est. vs measured flow - ' + sub + ' best.png')\n",
    "plt.show()\n",
    "\n",
    "plt.boxplot(best_df_predict['error (%)'])\n",
    "plt.ylabel('Error in est. flow (%)')\n",
    "plt.title('Best')\n",
    "plt.savefig(save_path + 'Est. error - ' + sub + ' best.png')\n",
    "plt.show()\n",
    "\n",
    "print('Best variables: ', best_var)\n",
    "print('Best r2: ', best_r2)\n",
    "print('Best accuracy: ', best_accuracy, '%')\n",
    "print('Best avg error: ', best_avg_error, '%')\n",
    "print('Best median error: ', best_med_error, '%')\n",
    "\n",
    "print('THE END')    \n",
    "\n",
    "# export good, better, and best dataframes to Excel\n",
    "writer = pd.ExcelWriter(save_path + 'Estimated daily flow 2018 - sub' + sub + ' good.xlsx', engine='xlsxwriter')\n",
    "good_df_predict.to_excel(writer, index=False)\n",
    "worksheet = writer.sheets['Sheet1']\n",
    "worksheet.write('S2', 'Combo no.')\n",
    "worksheet.write('T2', combo_no)\n",
    "worksheet.write('S3', 'R2')\n",
    "worksheet.write('T3', good_r2)\n",
    "worksheet.write('S4', 'Accuracy, %')\n",
    "worksheet.write('T4', good_accuracy)\n",
    "worksheet.write('S5', 'Avg error, %')\n",
    "worksheet.write('T5', str(good_avg_error))\n",
    "worksheet.write('S6', 'Median error, %')\n",
    "worksheet.write('T6', good_med_error)\n",
    "for column in good_df_predict:\n",
    "    column_length = max(good_df_predict[column].astype(str).map(len).max(), len(column))\n",
    "    col_idx = good_df_predict.columns.get_loc(column)\n",
    "    worksheet.set_column(col_idx, col_idx, column_length)\n",
    "worksheet.set_column('A:A', 20)\n",
    "worksheet.set_column('S:S', 15)\n",
    "worksheet.insert_image('S7', save_path + 'Est. vs measured flow - ' + sub + ' good.png')\n",
    "worksheet.insert_image('S27', save_path + 'Est. error - ' + sub + ' good.png')\n",
    "writer.save()\n",
    "\n",
    "writer = pd.ExcelWriter(save_path + 'Estimated daily flow 2018 - sub' + sub + ' better.xlsx', engine='xlsxwriter')\n",
    "better_df_predict.to_excel(writer, index=False)\n",
    "worksheet = writer.sheets['Sheet1']\n",
    "worksheet.write('S2', 'Combo no.')\n",
    "worksheet.write('T2', combo_no)\n",
    "worksheet.write('S3', 'R2')\n",
    "worksheet.write('T3', better_r2)\n",
    "worksheet.write('S4', 'Accuracy, %')\n",
    "worksheet.write('T4', better_accuracy)\n",
    "worksheet.write('S5', 'Avg error, %')\n",
    "worksheet.write('T5', str(better_avg_error))\n",
    "worksheet.write('S6', 'Median error, %')\n",
    "worksheet.write('T6', better_med_error)\n",
    "for column in better_df_predict:\n",
    "    column_length = max(better_df_predict[column].astype(str).map(len).max(), len(column))\n",
    "    col_idx = better_df_predict.columns.get_loc(column)\n",
    "    worksheet.set_column(col_idx, col_idx, column_length)\n",
    "worksheet.set_column('A:A', 20)\n",
    "worksheet.set_column('S:S', 15)\n",
    "worksheet.insert_image('S7', save_path + 'Est. vs measured flow - ' + sub + ' better.png')\n",
    "worksheet.insert_image('S27', save_path + 'Est. error - ' + sub + ' better.png')\n",
    "writer.save()\n",
    "\n",
    "writer = pd.ExcelWriter(save_path + 'Estimated daily flow 2018 - sub' + sub + ' best.xlsx', engine='xlsxwriter')\n",
    "best_df_predict.to_excel(writer, index=False)\n",
    "worksheet = writer.sheets['Sheet1']\n",
    "worksheet.write('S2', 'Combo no.')\n",
    "worksheet.write('T2', combo_no)\n",
    "worksheet.write('S3', 'R2')\n",
    "worksheet.write('T3', best_r2)\n",
    "worksheet.write('S4', 'Accuracy, %')\n",
    "worksheet.write('T4', best_accuracy)\n",
    "worksheet.write('S5', 'Avg error, %')\n",
    "worksheet.write('T5', str(best_avg_error))\n",
    "worksheet.write('S6', 'Median error, %')\n",
    "worksheet.write('T6', best_med_error)\n",
    "for column in best_df_predict:\n",
    "    column_length = max(best_df_predict[column].astype(str).map(len).max(), len(column))\n",
    "    col_idx = best_df_predict.columns.get_loc(column)\n",
    "    worksheet.set_column(col_idx, col_idx, column_length)\n",
    "worksheet.set_column('A:A', 20)\n",
    "worksheet.set_column('S:S', 15)\n",
    "worksheet.insert_image('S7', save_path + 'Est. vs measured flow - ' + sub + ' best.png')\n",
    "worksheet.insert_image('S27', save_path + 'Est. error - ' + sub + ' best.png')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below is a more detailed approach by testing all possible unique combinations of variables, but requires way longer processing time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "# how many variables to include in each combination set? \n",
    "min_combo_no = 1\n",
    "max_combo_no = len(all_variables)\n",
    "total_combo = 0\n",
    "\n",
    "for combo_no in range (min_combo_no, max_combo_no):\n",
    "    combo = [\",\".join(map(str, comb)) for comb in combinations(all_variables, combo_no)]\n",
    "    \n",
    "    # probably need another loop function to go through each combination set in the list\n",
    "    # insert machine learning and other data processing codes here\n",
    "    \n",
    "    # print the number of combination sets in each list\n",
    "    #print(len(combo))\n",
    "    total_combo = total_combo + len(combo)\n",
    "\n",
    "print('total possible unique combinations: ', total_combo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using random grid model to improve base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "# the values below are somewhat selected arbitarily to cover a wide range of parameters\n",
    "\n",
    "# Number of trees in random forest\n",
    "#n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "\n",
    "# Number of features to consider at every split\n",
    "#max_features = ['auto', 'sqrt']\n",
    "\n",
    "# Maximum number of levels in tree\n",
    "#max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "#max_depth.append(None)\n",
    "\n",
    "# Minimum number of samples required to split a node\n",
    "#min_samples_split = [2, 5, 10]\n",
    "\n",
    "# Minimum number of samples required at each leaf node\n",
    "#min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "# Method of selecting samples for training each tree\n",
    "#bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "#random_grid = {'n_estimators': n_estimators,\n",
    "#               'max_features': max_features,\n",
    "#               'max_depth': max_depth,\n",
    "#               'min_samples_split': min_samples_split,\n",
    "#               'min_samples_leaf': min_samples_leaf,\n",
    "#               'bootstrap': bootstrap}\n",
    "\n",
    "#pprint(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "#rf = ske.RandomForestRegressor()\n",
    "\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "#rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=0, n_jobs = 1)\n",
    "# Fit the random search model\n",
    "#rf_random.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Y_pred = rf_random.predict(X_test)\n",
    "\n",
    "#print('training period explained variance:', explained_variance_score(Y_test, Y_pred))\n",
    "#print('training period mean abs error:',mean_absolute_error(Y_test, Y_pred))\n",
    "#print('training period mean squared error:',mean_squared_error(Y_test, Y_pred))\n",
    "#print('training period r2:',r2_score(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def evaluate(model, X_train, Y_train):\n",
    "#    predictions = model.predict(X_train)\n",
    "#    errors = abs(predictions - Y_train)\n",
    "#    mape = 100 * np.mean(errors / Y_train)\n",
    "#    accuracy = 100 - mape\n",
    "#    print('Model Performance')\n",
    "#    print('Average Error: {:0.4f} cms.'.format(np.mean(errors)))\n",
    "#    print('Accuracy = {:0.2f}%.'.format(accuracy))\n",
    "    \n",
    "#    return accuracy\n",
    "\n",
    "#class color:\n",
    "#    BLUE = '\\033[94m'\n",
    "#    BOLD = '\\033[1m'\n",
    "#    END = '\\033[0m'\n",
    "\n",
    "#print(color.BOLD + color.BLUE + 'For Base Model' + color.END)\n",
    "#base_accuracy = evaluate(rf_base, X_train, Y_train)\n",
    "\n",
    "#print(color.BOLD + color.BLUE + 'For Random Model' + color.END)\n",
    "#best_random = rf_random.best_estimator_\n",
    "#random_accuracy = evaluate(best_random, X_train, Y_train)\n",
    "\n",
    "#print(color.BOLD + color.BLUE + 'Base vs random model comparison' + color.END)\n",
    "#print('Improvement of {:0.2f}%.'.format( 100 * (random_accuracy - base_accuracy) / base_accuracy))\n",
    "\n",
    "#pprint(rf_random.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# if the random model performed better than the base model, then use the best random grid to create range of each hyperparameter\n",
    "\n",
    "# Number of trees in random forest\n",
    "#n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "\n",
    "# Number of features to consider at every split\n",
    "#max_features = ['sqrt']\n",
    "\n",
    "# Maximum number of levels in tree\n",
    "#max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "#max_depth.append(None)\n",
    "\n",
    "# Minimum number of samples required to split a node\n",
    "#min_samples_split = [2, 3, 4]\n",
    "\n",
    "# Minimum number of samples required at each leaf node\n",
    "#min_samples_leaf = [1, 2, 3]\n",
    "\n",
    "# Method of selecting samples for training each tree\n",
    "#bootstrap = [False]\n",
    "\n",
    "# Create the better grid\n",
    "#better_grid = {'n_estimators': n_estimators,\n",
    "#               'max_features': max_features,\n",
    "#               'max_depth': max_depth,\n",
    "#               'min_samples_split': min_samples_split,\n",
    "#               'min_samples_leaf': min_samples_leaf,\n",
    "#               'bootstrap': bootstrap}\n",
    "\n",
    "# We used randamized search to identify the better grid (i.e., narrow down the range for each hyperparameter)\n",
    "# Now we use the better grid to instantiate the grid search model\n",
    "#grid_search = GridSearchCV(estimator = rf, param_grid = better_grid, \n",
    "#                          cv = 3, n_jobs = 1, verbose = 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the grid search to the data\n",
    "#grid_search.fit(X_train, Y_train)\n",
    "#grid_search.best_params_\n",
    "\n",
    "#best_grid = grid_search.best_estimator_\n",
    "#grid_accuracy = evaluate(best_grid, X_train, Y_train)\n",
    "\n",
    "#print('Improvement of {:0.2f}%.'.format( 100 * (grid_accuracy - base_accuracy) / base_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Y_pred = rf_random.predict(X_test)\n",
    "\n",
    "#print('training period explained variance:', explained_variance_score(Y_test, Y_pred))\n",
    "#print('training period mean abs error:',mean_absolute_error(Y_test, Y_pred))\n",
    "#print('training period mean squared error:',mean_squared_error(Y_test, Y_pred))\n",
    "#print('training period r2:',r2_score(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict 2018 data, then compare against actual sample avg flow data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_test = df_weather[df_weather[\"Year\"].isin([2018])]\n",
    "#df_test = df_test.reset_index(drop=True)\n",
    "#df_test = pd.merge(df_weather, df_testraw, left_on='Date', right_on='Sample date', how='inner')\n",
    "\n",
    "# convert Sample date from timestamp to numeric because sklearn cannot process timestamp format\n",
    "#df_test['Date'] = pd.to_numeric(pd.to_datetime(df_test['Date']))\n",
    "#df_test = df_test[variables]\n",
    "\n",
    "# use base or random depending on model performance\n",
    "#Y_pred = rf_base.predict(df_test)\n",
    "#Y_pred = rf_random.predict(df_test)\n",
    "\n",
    "#df_predict = pd.DataFrame(Y_pred, columns=['flow_pred'])\n",
    "#df_predict = pd.merge(df_test, df_predict, left_index=True, right_index=True)\n",
    "\n",
    "# this is flow-weighted sample avg flow\n",
    "#df_actual = pd.read_excel('Test data 2018.xlsx', sheet_name = 'By subwatershed')\n",
    "#df_actual = df_actual[(df_actual['Site'] == 'Sub8') & (df_actual['Sample type'] == 'Base')]\n",
    "#df_actual = df_actual[['Sample date','Flow (cms)']]\n",
    "#df_actual = df_actual.dropna()\n",
    "\n",
    "#df_predict = pd.merge(df_actual, df_predict['flow_pred'], left_index=True, right_index=True)\n",
    "#df_predict['error (%)'] = round(((df_predict['Flow (cms)']-df_predict['flow_pred'])/df_predict['Flow (cms)']*100), 0)\n",
    "\n",
    "#df_predict['Sample date'] = pd.to_datetime(df_predict['Sample date'])\n",
    "\n",
    "#print('prediction period explained variance:',explained_variance_score(df_predict['Flow (cms)'], df_predict['flow_pred']))\n",
    "#print('prediction period mean abs error:',mean_absolute_error(df_predict['Flow (cms)'], df_predict['flow_pred']))\n",
    "#print('prediction period mean squared error:',mean_squared_error(df_predict['Flow (cms)'], df_predict['flow_pred']))\n",
    "#print('prediction period r2:',r2_score(df_predict['Flow (cms)'], df_predict['flow_pred']))\n",
    "\n",
    "#plt.scatter(df_predict['Flow (cms)'], df_predict['flow_pred'])\n",
    "#plt.xlabel('Actual flow (cms)')\n",
    "#plt.ylabel('Predicted flow (cms)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#writer = pd.ExcelWriter('predict flow 2018.xlsx')\n",
    "# write dataframe to excel\n",
    "#df_predict.to_excel(writer)\n",
    "# save the excel\n",
    "#writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
